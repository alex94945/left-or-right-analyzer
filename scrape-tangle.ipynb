{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a44a60-a770-4141-baf6-e1354c90568c",
   "metadata": {},
   "source": [
    "# Scrape data, label it, and write it to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d202ffe-a20e-4379-9a81-b5049850dc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more 'Load more posts' button found or timeout reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, ElementNotInteractableException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Setup ChromeDriver\n",
    "chrome_options = Options()\n",
    "# Additional options can be added if needed\n",
    "service = Service(executable_path=ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = \"https://www.readtangle.com/archive/\"\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "# Click \"Load more posts\" button until all articles are loaded\n",
    "while True:\n",
    "    try:\n",
    "        load_more_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.js-load-more'))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_button)\n",
    "        time.sleep(1)  # Let any animations finish\n",
    "        load_more_button.click()\n",
    "        time.sleep(2)  # Wait for more articles to load\n",
    "    except (TimeoutException, NoSuchElementException, ElementNotInteractableException):\n",
    "        print(\"No more 'Load more posts' button found or timeout reached.\")\n",
    "        break\n",
    "\n",
    "articles_data = []\n",
    "\n",
    "# Find all article links on the page\n",
    "article_links = driver.find_elements(By.CSS_SELECTOR, 'article a')\n",
    "\n",
    "# Function to scrape article data\n",
    "def scrape_article_data(article_url):\n",
    "    # Open new tab\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "    # Switch to the new tab\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    driver.get(article_url)\n",
    "\n",
    "    # Wait for the article to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "    # Scrape the article title and text sections here\n",
    "    title = driver.find_element(By.CSS_SELECTOR, 'h1').text.strip()\n",
    "    \n",
    "    # Your existing function to get text after h3 until hr\n",
    "    # Ensure get_texts_after_h3_until_hr function is defined outside the loop\n",
    "    texts_by_section = {h3_id: get_texts_after_h3_until_hr(h3_id) for h3_id in h3_ids}\n",
    "\n",
    "    # Close the current tab\n",
    "    driver.close()\n",
    "    # Switch back to the first tab\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    return {\"title\": title, \"texts_by_section\": texts_by_section}\n",
    "\n",
    "# IDs of the h3 elements to scrape texts for\n",
    "h3_ids = [\n",
    "    \"todays-topic\",\n",
    "    \"what-the-right-is-saying\",\n",
    "    \"what-the-left-is-saying\"\n",
    "]\n",
    "\n",
    "for link in article_links:\n",
    "    article_url = link.get_attribute('href')\n",
    "    article_data = scrape_article_data(article_url)\n",
    "    articles_data.append(article_data)\n",
    "\n",
    "    # No need to go back since we are not leaving the main page\n",
    "\n",
    "# Process and save the scraped data\n",
    "json_data = json.dumps(articles_data, indent=4)\n",
    "print(json_data)\n",
    "\n",
    "# Optionally, save to a file\n",
    "# with open('articles_data.json', 'w') as f:\n",
    "#     f.write(json_data)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eef32fac-ac60-48b9-82f4-44ea7eb383f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles_data.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda6222-4ee4-4d9e-a1b0-e6e94a87fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check we have all (most) of the expected titles (most recent & oldest few match up)\n",
    "parsed_json = json.loads(json_data)\n",
    "\n",
    "for e in parsed_json:\n",
    "    print(e['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc5d9ccb-a06b-44dc-9d7f-d2c14ddf8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data = json.loads(articles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3da01e1d-4fd6-4936-923d-dd2cdf9fa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `articles_data` is loaded from your JSON file\n",
    "# with open('your_file.json', 'r') as f:\n",
    "#     articles_data = json.load(f)\n",
    "\n",
    "labeled_data = []\n",
    "\n",
    "for article in articles_data:\n",
    "    for section_key, paragraphs in article[\"texts_by_section\"].items():\n",
    "        # Combine all paragraphs into a single text, ensuring it reads smoothly.\n",
    "        combined_text = \" \".join(paragraphs).replace(\"\\\\u2019\", \"'\").replace(\"\\\\u201c\", \"\\\"\").replace(\"\\\\u201d\", \"\\\"\")\n",
    "        \n",
    "        # Assign labels based on section_key\n",
    "        if section_key == \"todays-topic\":\n",
    "            label = \"neutral\"\n",
    "        elif section_key == \"what-the-right-is-saying\":\n",
    "            label = \"right-leaning\"\n",
    "        elif section_key == \"what-the-left-is-saying\":\n",
    "            label = \"left-leaning\"\n",
    "        \n",
    "        # Append combined text and label to labeled_data\n",
    "        labeled_data.append({\"text\": combined_text, \"label\": label})\n",
    "\n",
    "# Now, labeled_data is ready to be used for training. It's also clean and consolidated.\n",
    "# Optionally, you can write this out to a file for inspection or further processing.\n",
    "with open('cleaned_labeled_data.json', 'w') as file:\n",
    "    json.dump(labeled_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653d0f4-8a5c-4e90-b94c-53d13fd63005",
   "metadata": {},
   "source": [
    "### Further Steps for Neural Network Training:\n",
    "\n",
    "*   **Tokenization and Encoding**: Convert the text into a format understandable by the network, usually involving converting text to sequences of integers representing tokens or words.\n",
    "    \n",
    "*   **Splitting Data**: Divide your data into training, validation, and test sets to evaluate the performance of your model accurately.\n",
    "    \n",
    "*   **Neural Network Architecture**: Design your neural network architecture. For sentiment analysis, recurrent neural networks (RNNs) or transformers are common choices due to their effectiveness in handling sequential data like text.\n",
    "    \n",
    "*   **Training**: Train your neural network on the processed and labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3a764-2836-4150-b685-6bbefa8833e5",
   "metadata": {},
   "source": [
    "# Preprocess the data for training, test, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bef02f-f796-439e-ab75-6d2e1baf74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the labeled data from file\n",
    "with open('cleaned_labeled_data.json', 'r') as file:\n",
    "    labeled_data = json.load(file)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "texts = [article[\"text\"] for article in labeled_data]\n",
    "labels = [article[\"label\"] for article in labeled_data]\n",
    "\n",
    "# Convert string labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_enc = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenize, encode, and pad sequences in the list of texts\n",
    "max_length = 512  # Define the maximum sequence length for BERT\n",
    "encoding = tokenizer(texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = encoding['input_ids']\n",
    "attention_masks = encoding['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels_enc, random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, attention_masks, random_state=42, test_size=0.1)\n",
    "\n",
    "# Ensure all inputs and labels are torch tensors\n",
    "train_inputs = train_inputs\n",
    "validation_inputs = validation_inputs\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = train_masks\n",
    "validation_masks = validation_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c96819-ab81-4952-9838-ad485e2d52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_inputs[0]\n",
    "decoded_example = tokenizer.decode(example)\n",
    "decoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d1208e-8ac3-46f8-b615-91fe7d104c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    problem_type=\"single_label_classification\",\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12fd1423-bf77-43db-9415-caf7ec4c63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, masks, labels):\n",
    "        self.inputs = inputs\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx].clone().detach(),\n",
    "            'attention_mask': self.masks[idx].clone().detach(),\n",
    "            'labels': self.labels[idx].clone().detach()\n",
    "        }\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = CustomDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = CustomDataset(validation_inputs, validation_masks, validation_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccee2bea-04e9-4bb7-8da6-dc2fdc3cb06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/test-together/test-together/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import EvalPrediction, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # p.predictions are logits from the model\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=p.label_ids, y_pred=preds)\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true=p.label_ids, y_pred=preds, average='weighted')\n",
    "    \n",
    "    # Return a dictionary of metrics\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",   # Evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",         # Save at the end of each epoch to match evaluation strategy\n",
    "    load_best_model_at_end=True,   # Load the best model at the end based on metric\n",
    "    metric_for_best_model=\"f1\"     # Specify the metric to use for loading the best model\n",
    ")\n",
    "\n",
    "# Assuming compute_metrics function is defined correctly according to your task\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,  # Passing tokenizer to ensure correct padding\n",
    "    compute_metrics=compute_metrics  # Define your metric computation function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8372922c-17b9-4a75-923b-a8e765456ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.masks[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 18:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.087935</td>\n",
       "      <td>0.969101</td>\n",
       "      <td>0.969063</td>\n",
       "      <td>0.971842</td>\n",
       "      <td>0.969101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.049727</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.988764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.032885</td>\n",
       "      <td>0.985955</td>\n",
       "      <td>0.985955</td>\n",
       "      <td>0.986550</td>\n",
       "      <td>0.985955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.masks[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.masks[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=0.1806035109050572, metrics={'train_runtime': 1095.5566, 'train_samples_per_second': 8.76, 'train_steps_per_second': 1.095, 'total_flos': 2525099469935616.0, 'train_loss': 0.1806035109050572, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c938ca-d0f5-4c85-8f37-4eac07396f17",
   "metadata": {},
   "source": [
    "### Results from 1st training attempt\n",
    "| Epoch | Training Loss | Validation Loss | Accuracy | F1       | Precision | Recall   |\n",
    "|-------|---------------|-----------------|----------|----------|-----------|----------|\n",
    "| 1     | 0.127100      | 0.087935        | 0.969101 | 0.969063 | 0.971842  | 0.969101 |\n",
    "| 2     | 0.005000      | 0.049727        | 0.988764 | 0.988764 | 0.988764  | 0.988764 |\n",
    "| 3     | 0.003000      | 0.032885        | 0.985955 | 0.985955 | 0.986550  | 0.985955 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e27a39-98cf-488c-993a-eac70a684717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.masks[idx], dtype=torch.long),\n",
      "/var/folders/_6/mwhvl8ls6xs1sr1_fcjk2fww0000gn/T/ipykernel_1872/2022800883.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.04972740635275841,\n",
       " 'eval_accuracy': 0.9887640449438202,\n",
       " 'eval_f1': 0.9887640449438202,\n",
       " 'eval_precision': 0.9887640449438202,\n",
       " 'eval_recall': 0.9887640449438202,\n",
       " 'eval_runtime': 11.1647,\n",
       " 'eval_samples_per_second': 31.886,\n",
       " 'eval_steps_per_second': 4.031,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff332b29-e1d3-495c-b946-eaa4ea54447e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73406745-2757-4864-b957-7aeea13a43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The case against Israel is morally bankrupt.\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b23a9331-cb30-4174-b585-7cd96693c414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33950b3a-06ce-4621-8bb7-2d50f5a4bce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right-leaning']\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0f9b5-d451-4c15-ad74-d202a5d2fee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da6f43-31a8-479a-a144-fa428a26cfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
